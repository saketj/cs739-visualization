{
  "topic": "DISK",
  "nodes": [
    {
      "id": 1,
      "label": "Bianca'07",
      "importance": 120,
      "paper-title": "Schroeder, B., & Gibson, G. A. (2007, February). Disk failures in the real world: What does an mttf of 1, 000, 000 hours mean to you?. In FAST (Vol. 7, pp. 1-16).",
      "download-link": "https://www.usenix.org/legacy/events/fast07/tech/schroeder/schroeder_html/index.html",
      "paper-highlights": "<ul><li><b>Data:</b> The authors looked at 100000 drives including HPC clusters at Los Alamos and the Pittsburgh Supercomputer Center, as well as several unnamed internet services providers. The drives had different workloads, different definitions of failure and different levels of data collection.</li><li><b>Vendor MTBF reliability: </b> While the MTTFs on the data sheets suggested an annual failure rate of no more than 0.88%, in the field, annual disk replacement rates typically exceed 1%, with 2-4% common and up to 13% observed on some systems. </li><li><b>No bathtub curve:</b> The disk failures were found to increase constantly with age, rather than setting in after a nominal life time of five years.</li><li><b>High-end enterprise drives versus consumer drives:</b> The authors observed little difference in replacement rates between SCSI, FC and SATA drives, potentially an indication that disk-independent factors, such as operating conditions, affect replacement rates more than component specific factors </li><li><b>Independence of drive failures in an array?</b> The distribution of time between disk replacements exhibits decreasing hazard rates, that is, the expected remaining time until the next disk was replaced grows with the time it has been since the last disk replacement.  one array drive failure means a much higher likelihood of another drive failure. The longer since the last failure, the longer to the next failure.</li></ul>",
      "paper-observations": "<ul><li>The devices we characterized in this study showed variation both within a block and over time in terms of power consumption, latency, and error rates.</li><li>Our data also show that the values manufacturers provide in publicly available datasheets often tell only part of the story, and that actual performance can be significantly worse and highly variable.</li> <li>Our application case studies demonstrate that by looking beyond the datasheets manufacturers provide, we can make significant improvements to flash-based storage devices.</li><li>Exploiting two of the effects we measured enabled us to significantly decrease latency for critical IO requests and extend the effective lifetimes of chips.</li></ul>"
    },
    {
      "id": 2,
      "label": "Bairavasundaram'07",
      "importance": 45,
      "paper-title": "Bairavasundaram, L. N., Goodson, G. R., Pasupathy, S., & Schindler, J. (2007, June). An analysis of latent sector errors in disk drives. In ACM SIGMETRICS Performance Evaluation Review (Vol. 35, No. 1, pp. 289-300). ACM",
      "download-link": "http://research.cs.wisc.edu/wind/Publications/latent-sigmetrics07.pdf",
      "paper-highlights": "<ul><li><b>Data:</b> The authors analyzed the error logs on over 50,000 arrays covering 1.53 million enterprise and consumer drives disks.</li><li>Latent Sector Errors (LSE) is defined as: when a particular disk sector cannot be read or written, or when there is an uncorrectable ECC error. Any data previously stored in the sector is lost.</li><li><b>File system implications:</b> File systems rely on disk-based data structures to keep track of your stuff. One of the key findings of the team is that disk errors tend to congregate near each other. File systems that replicate critical data across the disk are much less likely to lose data than those, like ReiserFS, place critical structures in one contiguous area.</li> <li><b>Enterprise vs consumer drive:</b> The higher LSE rate increase for aging consumer drives suggests that enterprise drives are higher quality. Or perhaps their error correction is better.</li><li><b>Several factors that contribute to LSE: </b><ul><li>Size: As disk size increases, so does the fraction of disks with LSE.</li><li>Age: LSE rates climbed with age. 20% of some – but not all – consumer disks had LSE after 24 months. Rates climbed faster for consumer drives than for enterprise drives.</li><li>Vendor: They also found that some vendors had much higher LSE than others.</li><li>Errors: A drive that develops one error is much more likely to develop a second. The second error is likely to be close to the first error. Once a drive develops an error, both enterprise and consumer drives are equally likely to develop a 2nd error.</li></ul><li></ul>",
      "paper-observations": "<ul> <li><strong>Observation 1:</strong> We observe that SSDs go through several distinct failure periods &ndash; early detection, early failure, usable life, and wearout &ndash; during their lifecycle, corresponding to the amount of data written to flash chips.</li> <li><strong>Observation 2:</strong> We find that the effect of read disturbance errors is not a predominant source of errors in the SSDs we examine.</li> <li><strong>Observation 3:</strong> Sparse data layout across an SSD&rsquo;s physical address space (e.g., non-contiguously allocated data) leads to high SSD failure rates; dense data layout (e.g., contiguous data) can also negatively impact reliability under certain conditions, likely due to adversarial access patterns.</li> <li><strong>Observation 4:</strong> Higher temperatures lead to increased failure rates, but do so most noticeably for SSDs that do not employ throttling techniques.&nbsp;</li> <li><strong>Observation 5:</strong> The amount of data reported to be written by the system software can overstate the amount of data actually written to flash chips, due to system-level buffering and wear reduction techniques.</li> </ul>"
    },
    {
      "id": 3,
      "label": "Pinheiro'07",
      "importance": 35,
      "paper-title": "Pinheiro, Eduardo, Wolf-Dietrich Weber, and Luiz André Barroso. Failure Trends in a Large Disk Drive Population. FAST. Vol. 7. 2007",
      "download-link": "https://static.googleusercontent.com/media/research.google.com/en//archive/disk_failures.pdf",
      "paper-highlights": "<ul><li><b>Data:</b> The authors looked at that company’s experience with more than 100,000 serial and parallel ATA consumer-grade HDDs running at speeds of 5400 to 7200 rpm. The disks were at least nine different models from seven different manufacturers, with sizes from 80 to 400 GB. </li><li><b>Age:</b> Disks have an annualized failure rate (AFR) of 3% for the first three months, dropping to 2% for the first year. In the second year the AFR climbed to 8% and stayed in the 6% to 9% range for years 3-5</li><li><b>Utilization: </b>It is commonly expected that high utilization correlates with high failure rates. However, the authors observed that only very young and very old disc groups appear to show the expected behavior. It is possible that failure modes that associated with higher utilization are more prominent early in drive’s lifetime. the drives that survive the infant mortality phase are the least susceptible to that failure mode. High correlation between utilization and failures has been based on extrapolations from manufacturers’ accelerated life experiments. Those experiments are likely to better model early life failure characteristics and as such they agree with the trend we observe for the young age groups</li><li><b>Temperature: </b>Temperature is often quoted as the most important environmental factor affecting disk drive reliability. Previous studies have indicated that temperature deltas as low as 15C can nearly double disk drive failure rates. Temperature effects only for the high end of our range and especially for older drives. In the lower and middle temperature ranges, higher temperatures are not associated with higher failure rates. The authors conclude that at moderate temperature ranges it is likely that there are other effects which affect failure rates much more strongly than temperatures do.</li><li><b>SMART: </b> 36% of the failed drives did not exhibit a single SMART-monitored failure. They concluded that SMART data is almost useless for predicting the failure of a single drive.</li></ul>",
      "paper-observations": "<ul> <li>Between 20&ndash;63% of drives experience at least one uncorrectable error during their first four years in the field, making uncorrectable errors the most common non-transparent error in these drives. Between 2&ndash;6 out of 1,000 drive days are affected by them.</li> <li>The majority of drive days experience at least one correctable error, however other types of transparent errors, i.e. errors which the drive can mask from the user, are rare compared to non-transparent errors.</li> <li>We find that RBER (raw bit error rate), the standard metric for drive reliability, is not a good predictor of those failure modes that are the major concern in practice. In particular, higher RBER does not translate to a higher incidence of uncorrectable errors.</li> <li>We find that UBER (uncorrectable bit error rate), the standard metric to measure uncorrectable errors, is not very meaningful. We see no correlation between UEs and number of reads, so normalizing uncorrectable errors by the number of bits read will artificially inflate the reported error rate for drives with low read count.</li> <li>Both RBER and the number of uncorrectable errors grow with PE cycles, however the rate of growth is slower than commonly expected, following a linear rather than exponential rate, and there are no sudden spikes once a drive exceeds the vendor&rsquo;s PE cycle limit, within the PE cycle ranges we observe in the field.</li> </ul>"
    },
    {
      "id": 4,
      "label": "Bairavasundaram'08",
      "importance": 20,
      "paper-title": "Bairavasundaram, L. N., Arpaci-Dusseau, A. C., Arpaci-Dusseau, R. H., Goodson, G. R., & Schroeder, B. (2008).An analysis of data corruption in the storage stack. ACM Transactions on Storage (TOS), 4(3), 8.",
      "download-link": "http://pages.cs.wisc.edu/~remzi/Classes/739/Fall2016/Papers/corruption-fast08.pdf",
      "paper-highlights": "<ul><li><b>Data: </b>The authors studied corruption instances that were logged in tens of thousands of storage systems for a period of 41 months starting in January 2004. These systems belonged to a range of different models, ran different versions of storage-controller software and contain many different models or versions of hardware components. The sample consisted of 1.53 million disk drives of 14 disk families and 31 distinct models.</li><li>Albeit not as common as latent sector errors, data corruption does happen; we observed more than 400,000 cases of checksum mismatches. For some drive models as many as 4% of drives develop checksum mismatches during the $17$ months examined. Similarly, even though they are rare, identity discrepancies and parity inconsistencies do occur. Protection offered by checksums and block identity information is therefore well-worth the extra space needed to store them.</li><li>A significant number (8% on average) of corruptions are detected during RAID reconstruction, creating the possibility of data loss. In this case, protection against double disk failures is necessary to prevent data loss. More aggressive scrubbing can speed the detection of errors, reducing the likelihood of an error during a reconstruction.</li><li>Although, the probability of developing a corruption is lower for enterprise class drives, once they develop a corruption, many more are likely to follow. Therefore, replacing an enterprise class drive on the first detection of a corruption might make sense (drive replacement cost may not be a huge factor since the probability of first corruption is low).</li><li>Some block numbers are much more likely to be affected by corruption than others, potentially due to hardware or firmware bugs that affect specific sets of block numbers. RAID system designers might be well advised to use staggered stripes such that the blocks that form the stripe are not stored at the same or nearby block number.</li><li>Strong spatial locality suggests that redundant data structures should be stored distant from each other.</li><li>The high degree of spatial and temporal locality also begs the question of whether many corruptions occur at the exact same time, perhaps when all blocks are written as part of the same disk request. This hypothesis suggests that important or redundant data structures that are used for recovering data on corruption should be written as part of different write requests spaced over time.</li><li>Strong spatial and temporal locality (over long time periods) also suggests that it might be worth investigating how the locality can be leveraged for smarter scrubbing, e.g. trigger a scrub before it's next scheduled time, when probability of corruption is high or selective scrubbing of an area of the drive that's likely to be affected.</li><li>Failure prediction algorithms in systems should take into account the correlation of corruption with other errors such as latent sector errors, increasing the probability of one error when an instance of the other is found. </li></ul>",
      "paper-observations": "Narayanan observations"
    },
    {
      "id": 5,
      "label": "Jiang'08",
      "importance": 10,
      "paper-title": "Jiang, W., Hu, C., Zhou, Y., & Kanevsky, A. (2008). Are disks the dominant contributor for storage failures?: A comprehensive study of storage subsystem failure characteristics. ACM Transactions on Storage (TOS), 4(3), 7.",
      "download-link": "https://www.usenix.org/legacy/event/fast08/tech/full_papers/jiang/jiang_html/",
      "paper-highlights": "<ul><li><b>Data:</b> The authors  analyzed the storage logs collected from about 39,000 storage systems commercially deployed at various customer sites. The data set covers a period of 44 months and includes about 1,800,000 disks hosted in about 155,000 storage shelf enclosures. Furthermore, our data covers a wide range of storage system classes, including near-line (backup), low-end, mid-range, and high-end systems.</li><li>Disk failures contribute to “only” 20-55 percent of storage subsystem failures.</li><li>Physical interconnect failures are a significant contributor – anywhere from 27-68% – of storage subsystem failures. Thus, to build highly reliable and available storage systems, only using resiliency mechanisms targeting disk failures (e.g., RAID) is not enough. We also need to build resiliency mechanisms such as redundant physical interconnects and self-checking protocol stacks to tolerate failures in these storage components.</li><li>Subsystem failure rates that use the same disk models show similar disk failure rates – but the subsystem failure rates vary significantly.</li><li>Each individual storage subsystem failure type and storage subsystem failure as a whole exhibit strong correlations, (i.e. after one failure, the probability of additional failures of the same type is higher). In addition, failures also exhibit bursty patterns in time distribution, (i.e. multiple failures of the same type tend to happen relatively close together). These results motivate a revisiting of current resiliency mechanisms such as RAID that assume independent failures. These results also motivate development of better resiliency mechanisms that can tolerate multiple correlated failures and bursty failure behaviors.</li><li>Storage subsystems configured with two independent interconnects experienced much (30-40%) lower AFRs than those with a single interconnect. This result indicates the importance of interconnect redundancy in the design of reliable storage systems.</li><li>Interconnect and protocol failure rates are much more bursty than disk failures. Some 48% of overall subsystem failure arrive at the same shelf within 10,000 seconds (~ 3 hours) of the previous failure.</li><li>As interconnect failures are so bursty, resilience mechanisms beyond RAID are required to achieve subsystem availability.</li></ul>",
      "paper-observations": "Zheng observations"
    },
    {
      "id": 6,
      "label": "Bianca'10",
      "importance": 10,
      "paper-title": "Schroeder, B., Damouras, S., & Gill, P. (2010). Understanding latent sector errors and how to protect against them. ACM Transactions on storage (TOS), 6(3), 9.",
      "download-link": "https://www.usenix.org/legacy/event/fast10/tech/full_papers/schroeder.pdf",
      "paper-highlights": "<ul><li><b>Data: </b>Studied the same date as by Bairavasundaram et. al.</li><li>The first part of the drive shows a higher concentration of errors than the remainder of the drive. Depending on the model, between 20% and 50% of all errors are located in the first 10% of the drive’s logical sector space. Similarly, for some models the end of the drive has a higher concentration. The authors speculate the areas of the drive with an increased concentration of errors might be are areas with different usage patterns, e.g. filesystems often store metadata at the beginning of the drive.</li><li><b>Does utilization matter?</b> The authors speculate that these areas that see a higher utilization  and the number of LSEs it develops correlate with either the number of reads or the number of writes that a drive sees. However, in other research at Google by Pinheiro et. al. they didn’t find such correlation. </li><li><b>Common distance between errors:</b> Between 20–60% of all errors have a neighbor within a distance of less than 10 sectors in logical sector space. However, we also observe that almost all models have pronounced “bumps” (parts where the CDF is steeper) indicating higher probability mass in these areas.</li><li>Errors that are close in space are also close in time</li></ul>",
      "paper-observations": " "
    },
    {
      "id": 7,
      "label": "Elerath'09",
      "importance": 20,
      "paper-title": "J. G. Elerath. Hard-disk drives: the good, the bad, and the ugly. Commun. ACM, 52(6), 2009",
      "download-link": "http://cacm.acm.org/magazines/2009/6/28493-hard-disk-drives-the-good-the-bad-and-the-ugly/fulltext",
      "paper-highlights": "Wang highlights",
      "paper-observations": " "
    }
  ],
  "edges": [
    {
      "from": 1,
      "to": 3,
      "edge-highlights": "<ul> <li>Recent work by Grupp et al. examined the BER of raw MLC flash chips (without performing error correction in the flash controller) in a controlled environment.</li> <li>They found the raw BER to vary from 1 &times; 10&minus;1 for the least reliable flash chips down to 1 &times; 10&minus;8 for the most reliable, with most chips having a BER in the 1 &times; 10&minus;6 to 1 &times; 10&minus;8 range.</li> <li><em><strong>Their study did not analyze the effects of the use of chips in SSDs under real workloads and system software.</strong></em></li> </ul>",
      "edge-observations": "<ul><li>This claims seem well supported.</li></ul>"
    },
    {
      "from": 1,
      "to": 2,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    },
    {
      "from": 1,
      "to": 4,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    },
    {
      "from": 1,
      "to": 5,
      "edge-highlights": "none-2-4",
      "edge-observations": "coming soon"
    },
    {
      "from": 1,
      "to": 7,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    },
    {
      "from": 2,
      "to": 4,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    },
    {
      "from": 2,
      "to": 5,
      "edge-highlights": "none-1-3",
      "edge-observations": "coming soon"
    },
    {
      "from": 2,
      "to": 6,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    },
    {
      "from": 3,
      "to": 2,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    },
    {
      "from": 4,
      "to": 5,
      "edge-highlights": "<div class = 'row'> <div class='col-md-12'><ul> <li>Schroder's study and Meza's study complement each other well, as they have very little overlap.</li> <li>The data in the Facebook study consists of a single snapshot in time for a fleet consisting of very young (in terms of the usage they have seen in comparison to their PE cycle limit) MLC drives and has information on uncorrectable errors only, while Schroder's&nbsp;study is based on per-drive time series data spanning drives&rsquo; entire lifecycle and includes detailed information on different types of errors, including correctable errors, and different types of hardware failures, as well as drives from different technologies (MLC, eMLC, SLC).</li> <li>As a result Schroder's&nbsp;study spans a broader range of error and failure modes, including wear-out effects across a drive&rsquo;s entire life. On the other hand, the Facebook study includes the role of some factors (temperature, bus power consumption, DRAM buffer usage) that our data does not account for.</li> </ul></div></div>",
      "edge-observations": "None yet"
    },
    {
      "from": 7,
      "to": 6,
      "edge-highlights": "none-5-6",
      "edge-observations": "coming soon"
    }
  ],
  "timeline": [
    {
      "title": "RAID '88",
      "publication": " ACM SIGMOD International Conference on Management of Data, 1988",
      "description": "Presented a case for redundant array of inexpensive disks (RAID) as a better option over a Single Large expensive disk (SLED) with improvements in performance, reliability, power consumption and scalability",
      "icon": "fa-facebook",
      "impact": "high"
    },
    {
      "title": "A comprehensive review of hard-disk drive reliability '99",
      "publication": "Annual Reliability and Maintainability Symposium, 1999",
      "description": "Showed the lifecycle failure pattern for hard drives as a 'bathtub model' <a href='https://research.fb.com/publications/a-large-scale-study-of-flash-memory-failures-in-the-field/' target='_blank'><img src='https://www.usenix.org/legacy/events/fast07/tech/schroeder/schroeder_html/img13.png' height='30%' width='50%'></a>",
      "icon": "fa-google",
      "impact": "medium"
    },
    {
      "title": "Bianca '07: Bathtub model is dead",
      "publication": "Published in FAST 2007",
      "description": "<a href='https://research.fb.com/publications/a-large-scale-study-of-flash-memory-failures-in-the-field/' target='_blank'><img src='https://www.usenix.org/legacy/events/fast07/tech/schroeder/schroeder_html/img14a.PNG' height='30%' width='80%'></a>",
      "icon": "fa-windows",
      "impact": "low"
    },
    {
      "title": "Pinheiro '07: No correlation between temperature and failures",
      "publication": "Published in FAST 2007",
      "description": ". <a href='https://research.fb.com/publications/a-large-scale-study-of-flash-memory-failures-in-the-field/' target='_blank'><img src='http://storagemojo.com/wp-content/uploads/2007/02/afr_temp.png' height='30%' width='50%'></a>",
      "icon": "fa-windows",
      "impact": "low"
    },
    {
      "title": "Bairavasundaram '08: Cheap is fragile",
      "publication": "Published in ACM Transactions on Storage 2008",
      "description": " <a href='https://research.fb.com/publications/a-large-scale-study-of-flash-memory-failures-in-the-field/' target='_blank'><img src='https://www.usenix.org/legacy/events/fast08/tech/full_papers/bairavasundaram/bairavasundaram_html/img27.png' height='30%' width='50%'></a> <a href='https://research.fb.com/publications/a-large-scale-study-of-flash-memory-failures-in-the-field/' target='_blank'><img src='https://www.usenix.org/legacy/events/fast08/tech/full_papers/bairavasundaram/bairavasundaram_html/img29.png' height='30%' width='50%'></a>",
      "icon": "fa-windows",
      "impact": "low"
    }
  ]
}